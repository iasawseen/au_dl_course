{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !! unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MAFSAEDVLKEYDRRRRMEALLLSLYYPNDRKLLDYKEWSPPRVQV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSIIGATRLQNDKSDTYSAGPCYAGGCSAFTPRGTCGKDWDLGEQT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MQNPLPEVMSPEHDKRTTTPMSKEANKFIRELDKKPGDLAVVSDFV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MDSLNEVCYEQIKGTFYKGLFGDFPLIVDKKTGCFNATKLCVLGGK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MEAKNITIDNTTYNFFKFYNINQPLTNLKYLNSERLCFSNAVMGKI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Sequences\n",
       "0  MAFSAEDVLKEYDRRRRMEALLLSLYYPNDRKLLDYKEWSPPRVQV...\n",
       "1  MSIIGATRLQNDKSDTYSAGPCYAGGCSAFTPRGTCGKDWDLGEQT...\n",
       "2  MQNPLPEVMSPEHDKRTTTPMSKEANKFIRELDKKPGDLAVVSDFV...\n",
       "3  MDSLNEVCYEQIKGTFYKGLFGDFPLIVDKKTGCFNATKLCVLGGK...\n",
       "4  MEAKNITIDNTTYNFFKFYNINQPLTNLKYLNSERLCFSNAVMGKI..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_df = pd.read_table('data/family_classification_sequences.tab')\n",
    "seq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_codones(sseq):\n",
    "    crop = len(sseq) % 3\n",
    "    cropped_seq = sseq[:-crop] if crop > 0 else sseq\n",
    "\n",
    "    return [cropped_seq[i:i+3] for i in range(0, len(cropped_seq), 3)]\n",
    "\n",
    "def seq_to3(seq):\n",
    "    splittings = [make_codones(seq[i:]) for i in range(3)]\n",
    "    return splittings\n",
    "\n",
    "def create_all_codones(df):\n",
    "    codones = []\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        row = df.iloc[i, :][0]\n",
    "        codones.extend(seq_to3(row))\n",
    "\n",
    "    return codones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_or_create(read_path, producer):\n",
    "    if os.path.isfile(read_path):\n",
    "        print('reading', read_path)\n",
    "        with open(read_path, 'rb') as fp:\n",
    "            return pickle.load(fp)\n",
    "    result = producer()\n",
    "    print('saving', read_path)\n",
    "    with open(read_path, 'wb') as fp:\n",
    "        pickle.dump(result, fp)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data/all_codones.pickle\n"
     ]
    }
   ],
   "source": [
    "all_codones = read_or_create(read_path='data/all_codones.pickle',\n",
    "                             producer= lambda: create_all_codones(seq_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sample(index_words_list, context_window_size):\n",
    "    \"\"\" Form training pairs according to the skip-gram model. \"\"\"\n",
    "    \n",
    "    size = len(index_words_list)\n",
    "    ran = list(range(size))\n",
    "    random.shuffle(ran)\n",
    "    \n",
    "#     for index_words in index_words_list:\n",
    "#         for index, center in enumerate(index_words):\n",
    "\n",
    "    for i in ran:\n",
    "        for index, center in enumerate(index_words_list[i]):\n",
    "            context = random.randint(1, context_window_size)\n",
    "            \n",
    "            # get a random target before the center word\n",
    "#             for target in index_words[max(0, index - context): index]:\n",
    "            for target in index_words_list[i][max(0, index - context): index]:\n",
    "                yield center, target\n",
    "            \n",
    "            # get a random target after the center wrod\n",
    "#             for target in index_words[index + 1: index + context + 1]:\n",
    "            for target in index_words_list[i][index + 1: index + context + 1]:\n",
    "                yield center, target\n",
    "\n",
    "\n",
    "def get_batch(iterator, batch_size):\n",
    "    \"\"\" Group a numerical stream into batches and yield them as Numpy arrays. \"\"\"\n",
    "    while True:\n",
    "        center_batch = np.zeros(batch_size, dtype=np.int32)\n",
    "        target_batch = np.zeros([batch_size, 1], dtype=np.int32)\n",
    "        \n",
    "        for index in range(batch_size):\n",
    "            center_batch[index], target_batch[index] = next(iterator)\n",
    "        \n",
    "        yield center_batch, target_batch\n",
    "\n",
    "\n",
    "def flatten(x):\n",
    "    return [item for sublist in x for item in sublist]\n",
    "\n",
    "\n",
    "def cod_to_dict(cod, dictionary):\n",
    "    return [dictionary[key] for key in cod]\n",
    "\n",
    "\n",
    "def make_dictionary(all_codones):\n",
    "    flat_codones = flatten(all_codones)\n",
    "#     unique_codones = set(flat_codones)\n",
    "    \n",
    "    counter = Counter(flat_codones)\n",
    "    \n",
    "    unique_codones = list(set(flat_codones))\n",
    "    unique_codones = sorted(unique_codones, key=lambda x: counter[x], \n",
    "                            reverse=False)\n",
    "    \n",
    "#     print(unique_codones[:6])\n",
    "    \n",
    "#     count = 0\n",
    "\n",
    "#     for key in unique_codones:\n",
    "#         print(key, counter[key])\n",
    "#         count += 1\n",
    "#         if count > 6:\n",
    "#             break\n",
    "            \n",
    "    dictionary = {cod: i for i, cod in enumerate(unique_codones)}\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def process_data(all_codones, dictionary, batch_size, skip_window):\n",
    "    cod_dicts = [cod_to_dict(cod, dictionary) for cod in all_codones]\n",
    "    single_gen = generate_sample(cod_dicts, context_window_size=skip_window)\n",
    "    batch_gen = get_batch(single_gen, batch_size=batch_size)\n",
    "    return batch_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_codones = np.array([[['abc', 'gec', 'agc'], ['get', 'ghe', 'acg']]])\n",
    "\n",
    "dictionary = make_dictionary(all_codones)\n",
    "\n",
    "print(len(dictionary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "SKIP_WINDOW = 10  # the context window\n",
    "\n",
    "batch_gen = process_data(all_codones, dictionary, BATCH_SIZE, SKIP_WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape, name):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def bias_variable(shape, name):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "class SkipGramModel:\n",
    "    \"\"\" Build the graph for word2vec model \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, batch_size, num_sampled, learning_rate):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_sampled = num_sampled\n",
    "        self.lr = learning_rate\n",
    "        self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "        self.saver = tf.train.Saver()  # defaults to saving all variables - in this case embed_matrix, nce_weight, nce_bias\n",
    "\n",
    "    def _create_placeholders(self):\n",
    "        with tf.name_scope(\"data\"):\n",
    "            self.center_words = tf.placeholder(tf.int32, shape=[self.batch_size], name='center_words')\n",
    "            self.target_words = tf.placeholder(tf.int32, shape=[self.batch_size, 1], name='target_words')\n",
    "\n",
    "    def _create_fully_conn(self, input_tensor, from_n, to_n, layer_name):\n",
    "        with tf.name_scope(layer_name):\n",
    "            W_fc = weight_variable(shape=[from_n, to_n], name=layer_name + '_W')\n",
    "            b_fc = bias_variable(shape=[to_n], name=layer_name + '_b')\n",
    "            h_fc = tf.nn.tanh(tf.add(tf.matmul(input_tensor, W_fc), b_fc))\n",
    "            return h_fc\n",
    "           \n",
    "    def _create_fully_conn_logit(self, input_tensor, from_n, to_n, layer_name):\n",
    "        with tf.name_scope(layer_name):\n",
    "            W_fc = weight_variable(shape=[from_n, to_n], name=layer_name + '_W')\n",
    "            b_fc = bias_variable(shape=[to_n], name=layer_name + '_b')\n",
    "            h_fc = tf.add(tf.matmul(input_tensor, W_fc), b_fc)\n",
    "            return h_fc        \n",
    "    \n",
    "    def _create_embedding(self):\n",
    "        with tf.name_scope(\"embed\"):\n",
    "            self.embed_matrix = tf.Variable(tf.random_uniform([self.vocab_size,\n",
    "                                                               self.embed_size], -1.0, 1.0),\n",
    "                                            name='embed_matrix')\n",
    "            \n",
    "#             fc1_ratio = 8\n",
    "#             fc2_ratio = 8\n",
    "#             fc3_ratio = 8\n",
    "\n",
    "            self.embed_matrix = self._create_fully_conn(self.embed_matrix, self.embed_size, \n",
    "                                                        self.embed_size, 'fc1')\n",
    "#             self.embed_matrix = self._create_fully_conn(self.embed_matrix, self.embed_size * fc1_ratio, \n",
    "#                                                         self.embed_size * fc2_ratio, 'fc2')\n",
    "#             self.embed_matrix = self._create_fully_conn(self.embed_matrix, self.embed_size * fc2_ratio, \n",
    "#                                                         self.embed_size * fc3_ratio, 'fc3')\n",
    "#             self.embed_matrix = self._create_fully_conn(self.embed_matrix, self.embed_size * fc3_ratio, \n",
    "#                                                         self.embed_size, 'fc4')\n",
    "\n",
    "            \n",
    "#             self.embed_matrix = tf.Variable(tf.truncated_normal([self.vocab_size, self.embed_size], \n",
    "#                                                                 stddev=1.0 / (self.embed_size ** 0.5)),\n",
    "#                                             name='embed_matrix')\n",
    "            \n",
    "#             self.embed_matrix = self._create_fully_conn(self.embed_matrix, self.embed_size, 100, 'fc')\n",
    "\n",
    "    def _create_loss(self):\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            embed = tf.nn.embedding_lookup(self.embed_matrix, self.center_words, name='embed')\n",
    "            \n",
    "            fc1_ratio = 32\n",
    "            fc2_ratio = 4\n",
    "            fc3_ratio = 4\n",
    "            \n",
    "            fc1 = self._create_fully_conn(embed, self.embed_size, self.embed_size * fc1_ratio, 'fc1')\n",
    "            fc2 = self._create_fully_conn(fc1, self.embed_size * fc1_ratio, self.embed_size * fc2_ratio, 'fc2')\n",
    "            fc3 = self._create_fully_conn(fc2, self.embed_size * fc2_ratio, self.embed_size * fc3_ratio, 'fc3')\n",
    "            fc4 = self._create_fully_conn(fc3, self.embed_size * fc3_ratio, self.embed_size, 'fc4')\n",
    "#             fc5 = self._create_fully_conn(embed, self.embed_size, self.embed_size, 'fc5')\n",
    "            \n",
    "            # construct variables for NCE loss\n",
    "            nce_weight = tf.Variable(tf.truncated_normal([self.vocab_size, self.embed_size],\n",
    "                                                         stddev=1.0 / (self.embed_size ** 0.5)),\n",
    "                                     name='nce_weight')\n",
    "            \n",
    "#             nce_bias = tf.Variable(tf.zeros([self.vocab_size]), name='nce_bias')\n",
    "            nce_bias = tf.Variable(tf.truncated_normal([self.vocab_size], stddev=1.0 / (self.embed_size ** 0.5)), \n",
    "                                   name='nce_bias')\n",
    "            \n",
    "            # define loss function to be NCE loss function\n",
    "            self.loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight,\n",
    "                                                      biases=nce_bias,\n",
    "                                                      labels=self.target_words,\n",
    "#                                                       inputs=embed,\n",
    "                                                      inputs=fc4,\n",
    "                                                      num_sampled=self.num_sampled,\n",
    "                                                      num_classes=self.vocab_size), name='loss')\n",
    "\n",
    "    def _create_optimizer(self):\n",
    "#         self.optimizer = tf.train.GradientDescentOptimizer(self.lr).minimize(self.loss, global_step=self.global_step)\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.lr).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "        \n",
    "    def _create_summaries(self):\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar(\"loss\", self.loss)\n",
    "            tf.summary.histogram(\"histogram loss\", self.loss)\n",
    "            # because you have several summaries, we should merge them all\n",
    "            # into one op to make it easier to manage\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "\n",
    "    def build_graph(self):\n",
    "        \"\"\" Build the graph for our model \"\"\"\n",
    "        self._create_placeholders()\n",
    "        self._create_embedding()\n",
    "        self._create_loss()\n",
    "        self._create_optimizer()\n",
    "        self._create_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 9424\n",
    "EMBED_SIZE = 100  # dimension of the word embedding vectors\n",
    "NUM_SAMPLED = 16  # Number of negative examples to sample.\n",
    "LEARNING_RATE = .05\n",
    "NUM_TRAIN_STEPS = 100000\n",
    "SKIP_STEP = 2000\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    model = SkipGramModel(VOCAB_SIZE, EMBED_SIZE, BATCH_SIZE, NUM_SAMPLED, LEARNING_RATE)\n",
    "    model.build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dir(path):\n",
    "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, batch_gen, num_train_steps, learning_rate, skip_step):\n",
    "    make_dir('checkpoints')\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.25)\n",
    "    \n",
    "    with tf.Session(graph=g, config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "#         if that checkpoint exists, restore from checkpoint\n",
    "        if ckpt and os.path.isfile(ckpt.model_checkpoint_path):\n",
    "            model.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "        total_loss = 0.0  # we use this to calculate late average loss in the last SKIP_STEP steps\n",
    "        writer = tf.summary.FileWriter('improved_graph/lr' + str(learning_rate), sess.graph)\n",
    "        initial_step = model.global_step.eval()\n",
    "        for index in range(initial_step, initial_step + num_train_steps):\n",
    "            centers, targets = next(batch_gen)\n",
    "            feed_dict = {model.center_words: centers, model.target_words: targets}\n",
    "            loss_batch, _, summary = sess.run([model.loss, model.optimizer, model.summary_op],\n",
    "                                              feed_dict=feed_dict)\n",
    "            writer.add_summary(summary, global_step=index)\n",
    "            total_loss += loss_batch\n",
    "            if (index + 1) % skip_step == 0:\n",
    "                print('Average loss at step {}: {:5.1f}'.format(index, total_loss / skip_step))\n",
    "                total_loss = 0.0\n",
    "                model.saver.save(sess, 'checkpoints/skip-gram', index)\n",
    "\n",
    "        final_embed_matrix = sess.run(model.embed_matrix)\n",
    "        return final_embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embed_matrix = train_model(model, batch_gen, NUM_TRAIN_STEPS, LEARNING_RATE, SKIP_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "XX = tsne.fit_transform(final_embed_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df = pd.DataFrame(XX, columns=['x0', 'x1'])\n",
    "unique_codones = sorted(dictionary, key=dictionary.get)\n",
    "tsne_df['codone'] = list(unique_codones)\n",
    "tsne_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_tsne_df(df):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.title('unlabeled encoding', fontsize=20)\n",
    "    plt.scatter(df.x0, df.x1, s=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_df(tsne_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'data/acid_properties.csv'\n",
    "props = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def acid_dict(some_c, props):\n",
    "    prop_by_letter = [props[props.acid == let].iloc[:, 1:] for let in some_c]   \n",
    "    df_concat = pd.concat(prop_by_letter)\n",
    "    res = df_concat.mean()\n",
    "    dres = dict(res)\n",
    "    dres['acid'] = some_c\n",
    "    return dres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'data/all_acid_dicts.pickle'\n",
    "producer = lambda: [acid_dict(some_c, props) for some_c in tsne_df.codone]\n",
    "all_acid_dicts = read_or_create(save_path, producer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_acid_df = pd.DataFrame(all_acid_dicts)\n",
    "all_acid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = all_acid_df.join(tsne_df.set_index('codone'), on='acid')\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_embedding_properties(final_df):\n",
    "    plt.figure(figsize=(25, 20))\n",
    "    for i, p in enumerate(['hydrophobicity', 'mass', 'number_of_atoms', 'volume']):\n",
    "        plt.subplot(2,2,i+1)\n",
    "        plt.title(p, fontsize=25)\n",
    "        plt.scatter(final_df.x0, final_df.x1, c=final_df[p], s=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embedding_properties(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'data/nice_embed_tsne.csv'\n",
    "gensim_tsne_df = pd.read_csv(filename, index_col=0)\n",
    "gensim_tsne_df.columns = ['x0', 'x1', 'codone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_df(gensim_tsne_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df_nice = all_acid_df.join(gensim_tsne_df.set_index('codone'), on='acid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embedding_properties(final_df_nice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Improve SkipGramModel to archive better embedding for amino acids codones. Visualize your space in the similar style as on the bottom example. You are only allowed to use vanilla tensorflow for this task.\n",
    "\n",
    "Bonus task(no credit): visualize your embedding space in similar manner as minst example: https://www.tensorflow.org/versions/r0.12/how_tos/embedding_viz/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
